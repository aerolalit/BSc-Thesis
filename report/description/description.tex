  This is the technical core of the thesis. Here you lay out your how  you answered your research question, you specify your design of  experiments or simulations, point out difficulties that you  encountered, etc.  (target size: 3-4 pages)    \subsection{Data Description and Preprocessing}    \indent \indent  		The data set used for this experiment was obtained from Kaggle (a platform for predictive modelling and analytics competitions). The data set was provided by Google for a web traffic prediction competition.		 This data file consists of  number of views for 145063 Wikipedia pages on each date starting from 1$^{st}$ of July 2015 to 31$^{st}$ of December 2016. Each time series consists of data for the traffic generated by humans or bots or both from devices like mobile or desktop .		 For each date, the data contains a single integer value representing the number of views that a particular Wikipedia page received on that date.  The total length of each time series is 550. There are some missing values for some of the time series. In this experiment, we have ignored such time series for the sake of convenience. Out of 145063 time series, we have chosen the traffic generated for page named "2002 FIFA World Cup" on english version of Wikipedia i.e \url{https://en.wikipedia.org/} on desktop devices by all types of agents including humans and bots. \\		 		% Some of the raw time series data chosen for this experiment are visualized in Figure \ref{fig:raw}\subref{fig:raw_12} and Figure \ref{fig:raw}\subref{fig:raw_15}.		 		\begin{figure}		     \centering		     \includegraphics[width = 15cm]{./description/images/rawSignal}			   			   \caption{Raw data which has number of views of Wikipedia Pages on each date from 1$^{st}$ of July  2015 to 31$^{st}$ of December 2016. }			\label{fig:raw}			\end{figure}			For the preprocessing of the time-series following steps are followed:\subsubsection{Step 1}The time series data used in this experiment has very large values ($\approx  7000$) as seen in the the Figure \ref{fig:raw}.  Therefore, the time-series data is squeezed component wise such that the maximum value, $MAX$ of the time-series is squeezed to the new maximum value $max$. This is done by taking small ($< < 1$) power of each component in time series $\mathbf{s}(n) = (s(1),\hdots,s(n))$. The power $p$ that need to be raised to each component of time-series is calculated using Eq. \ref{eq:power}.  \begin{equation}\begin{split}		max &= MAX^p\\		p &= log_{MAX}(max)\end{split}\label{eq:power}\end{equation}  \begin{figure}[h]      % \centering      \begin{subfigure}[h]{0.5\textwidth}          \includegraphics[width=\textwidth]{./description/images/compare_squeezed}          \caption{ After taking component-wise power}          \label{fig:compare_squeezed}      \end{subfigure}       %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.         %(or a blank line to force the subfigure onto a new line)      \begin{subfigure}[h]{0.5\textwidth}          \includegraphics[width=\textwidth]{./description/images/compare_tanh}          \caption{After further taking the $tanh$ of the signal}          \label{fig:compare_tanh}      \end{subfigure}      \begin{subfigure}[h]{0.5\textwidth}         \includegraphics[width=\textwidth]{./description/images/compare_zoom_squeezed}         \caption{After further taking the $tanh$ of the signal}         \label{fig:compare_zoom_squeezed}     \end{subfigure}     \begin{subfigure}[h]{0.5\textwidth}         \includegraphics[width=\textwidth]{./description/images/compare_zoom_tanh}         \caption{After further taking the $tanh$ of the signal}         \label{fig:compare_zoom_tanh}     \end{subfigure}      \caption{Preprocessing steps}\label{fig:preprocessing}  \end{figure}    \subsubsection{Step 2}   The resultant time series from step 1 is further rescaled and shifted such that it has zero mean and unit variance. The time-series data is divided component wise by its standard deviation to make its variance unit.    \subsubsection{Step 3}\indent  The resultant time series from step 2 is further applied a $tanh$ function component wise. This strictly scales the data points in time series in the range of $-1$ to $1$.    \begin{figure}[h]      % \centering      \begin{subfigure}[h]{0.5\textwidth}          \includegraphics[width=\textwidth]{./description/images/squeezed}          \caption{ After taking component-wise power}          \label{fig:squeezed}      \end{subfigure}       %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.         %(or a blank line to force the subfigure onto a new line)      \begin{subfigure}[h]{0.5\textwidth}          \includegraphics[width=\textwidth]{./description/images/tanh}          \caption{After further taking the $tanh$ of the signal}          \label{fig:tanh}      \end{subfigure}\\                 \caption{Preprocessing steps}\label{fig:animals}  \end{figure}   In step 1, the higher values in signal is penalized more than the smaller ones. Therefore the higher peaks in the input signal are shortened and the smaller wiggles in the signal are scaled up. In  Fig. \ref{fig:preprocessing} \subref{fig:compare_squeezed} ( zoomed in version in Fig. \ref{fig:preprocessing} \subref{fig:compare_zoom_squeezed}) the yellow signal generated with the application of only step 2 has lesser wiggliness than the blue signal generated with the application of both step 1 and step 2. Even after the application of $tanh$ in step 3, the wiggliness in the blue signal generated by applying all three steps is higher than the yellow signal that is generated without applying step 1 (see Fig. \ref{fig:preprocessing} \subref{fig:compare_squeezed}, or zoomed in version in \ref{fig:preprocessing} \subref{fig:compare_zoom_squeezed} ). Patterns become more prominent for ESN with the increase in these wiggleness.  Applying $tanh$ function in step 3 further scales up the smaller wiggleness and scales down the higher peaks such that the values at any time point in signal is within the range of $-1$ to $1$. \subsection{Additional Input Signals}  \indent \indent By nature, the number of views that a particular Wikipedia Page receives vary according to  days in week and months in a year. For instance: A page related to weekend activities might receive a different proportion of views on weekends than on working days. Further, Wikipedia pages that on related subject matters might receives proportional number of views and could follow similar pattern of incoming web traffic. Therefore those related input signals were added to the ESN to improvise the prediction result. In particular we used following approach to find suitable additional input for the network. \\\\  \begin{enumerate}	 \item First of all a sine wave of period 7 was searched which has highest correlation with the main input signal. It was further preprocessed using the same routines as used for the main input signal. This sine wave signal was further scaled with a scaling factor $sf_{sine}$ and then input to the network.	 \item Then from the database of about 145 thousand time series top 20 time series are selected with has highest correlation with the main input signal. These signals are further preprocessed using the same preprocessing routines as used for preprocessing the main input signal. Finally each of these additional signals were a scaling factor $sf_{add}/NUM$, where $NUM$ is the total number of such additional inputs. To determine the correlation between two time series we have used Pearson's correlation coefficient.	  \end{enumerate}    \subsubsection{Pearson correlation coefficient}  Pearson's correlation coefficient is the measure of the linear correlation coefficient between two variables. It's value is in the range of -1 to 1. Value close to 1 indicate that the variables are highly correlated to each other and values close -1 indicates that the variables are negatively correlated to each other and values close to 0 indicates that the two variables are unrelated. Pearson coefficient between to time series $\mathbf{X} = (x_1, \hdots, x_m)$ and $\mathbf{Y} = (y_1, \hdots, y_m)$ can be computed using the Equation %\cite{eq:pearson}    \begin{equation}  \begin{split}	  \rho X,Y =& \frac{cov(X,Y)}{\sigma_X \sigma_Y}\\		=& \frac{\Sigma^{m}_{i=1}(x_i - \bar{x})(y_i - \bar{y})} {\sqrt{\Sigma^m_{i=1}(x_i - \bar{x}^2)}}  \end{split}   \label{eq:pearson}  \end{equation}    where:  \begin{itemize}	  \item $m$ is the length of time series	  \item $x_i, y_i$ are the data points in time series $X$ and $Y$ indexed with $i$	  \item $\bar{x_i} = \frac{1}{n}\Sigma_{i=1}^{n}x_i$  and analogously for $\bar{y}$	  	  \end{itemize}   So to give the insight of weeks and months, two additional input signals with periods 7 and 30 $\pm$ 2 were introduced to the network. These periodic signals were preprocessed (applied log10 and shifted by its mean) before feeding it to the network.		% Max &= \text{max}\left( \mathbf{s}(n)  \right) \\	% 	Max^p = \text{max}\left( \mathbf{s}(n)  \right) ^p\\	% 	max &= \text{max}( (s(1),\hdots,s(n)) % \end{multline}	% \begin{multiline}% 		max = \text{max}( \mathbf{s}(n)  ^p \\% 		max = \text{max}( (s(1),\hdots,s(n)) ^p% 	\end{multiline}%\subsection{Network Setup}\indent \indent    Under suitable conditions the network state becomes asymptotically independent of initial conditions and depends only on the input history, which is called the "Echo State Property". This means that all desired output signals can be build out of it's own "echos" in the Dynamic Reservoir. \cite{ESNinAudioProcessing}\\	We create a reservoir of $N = 99$ neurons. %The weights of each synaptic links connecting the neurons of reservoirs are chosen randomly from uniform distribution over (-0.5, 0.5). These weights are stored in  weight matrix $\mathbf{W}\in \mathbb{R}^{N\times N}$.	   The weights  for input links  	 and bias vector 	 are generated randomly from uniform distribution over (-0.5, 0.5) and stored in matrix $\mathbf{W}^{in} \in \mathbb{R}^{N\times K}$  and $\mathbf{B} \in \mathbb{R}^{N\times 1}$ respectively. Then 	 $\mathbf{W}$ matrix is normalized by its maximum eigenvalue. There are $K=22$ input neurons that receive each of the $21$ input signals and $L=31$ output neurons each of which makes $t=1,\hdots,31$ step ahead prediction.\\In order to achive good approximation of desired signal, the echo functions should provide a "rich" set of dynamics to combine from. The network should be prepared in a suitably "inhomogeneous" way to meet this demand. One simple method to prepare such a "rich reservoir" echo state network is to supply a network which is sparsely and randomly connected. Sparse connectivity provides for a relative decoupling of subnetworks, which encourages the development of individual dynamics \cite{shortTermMemory}. In this experiment, the network weight matrix, $\mathbf{W}$ has different weight regions to create different capability of short term memory so that the network can capture all type of trend (slow, medium and fast trends) in the training signal. The structure of weights matrix used is presented below:\\      \begin{center}	  \begin{tabular}{|l|r|c|}\hline		  \		  fast & $\approx$ 0 & =0 \\[5ex] \hline		  $\approx$ 0 & medium & $\approx$ 0 \\[5ex] \hline		  =0 & $\approx$ 0 & slow \\[5ex] \hline 	  \end{tabular}	  	  \end{center}Values in the weight matrix for in all the regions are drawn from the uniform distribution of range $-1$ to $1$ and scaled with a scaling factor. The scaling factors for the fast, medium and slow regions are $a, b \text{ and } c$ respectively such that $a \textgreater b \textgreater c$. For other regions the scaling factors are almost equal to zero or zero. These scaling factors are optimized during the training period for the better performance of the network.\\The spectral radius of the reservoir weight matrix \textbf{W} codetermines (i) the effective time constant of the echo state network (larger spectral radius implies slower decay of impulse response) and (ii) the amount of nonlinear interaction of input components through time (larger spectral radius implies longer-range interactions) \cite{Jaeger:2007}.  The weight matrix $\mathbf{W}_0$ is normalized to  $\mathbf{W}_1$ with it's spectral radius  by putting $\mathbf{W}_1 = \left| 1/\lambda _{max}\right| \mathbf{W}_0$ where $\lambda _{max}$ is the spectral radius of $\mathbf{W}_0$. Then the weight matrix $\mathbf{W_1}$ is scaled with a scaling factor $sf_W$ such that $\mathbf{W} = sf_W \mathbf{W_1}$. Then $\mathbf{W}$ has spectral radius of $sf_W$. The choice of the spectral radius $sf_W$ of the reservoir is crucial for the eventual success of ESN training. This is because $sf_W$ is  intimately connected to the intrinsic timescale of the dynamics of the reservoir state. Small $sf_W$ means that one has a fast dynamics reservoir and large $sf_W$ (i.e close to unity) means that one has slow reservoir. Also the input weight matrix $\mathbf{W}^{in}$  and bias vector $\mathbf{B}$ is scaled with a scaling factors $sf_Win$ and $sf_B$ respectively. \subsection{Performance Metrics}\label{performance_metrics}\indent \indentWe have used Normalized Root Mean Square Error (NRMSE) as a measure to evaluate the performance of the reservoir. NRMSE measures the difference between the predicted values $y$ and true values $r$. The values for $y$ and $r$ in this experiment are one dimensional. NRMSE for one dimensional $y$ and $r$ can be computed using the Equation \eqref{eq:nrmse}.\begin{equation}	NRMSE = \sqrt{\frac{\sum_{i=1}^n(y(i)-r(i))^2}{N \sigma ^2}}\label{eq:nrmse}\end{equation}where $N$ denotes the number of output data points, $\sigma^2$ denotes the variance of $y$ and $r$. NRMSE with its value close to one indicates that the compared two signal  $y$ and $r$ are completely unrelated to each other. NRMSE of value close to zero indicates that the model might have overfitted and could give bad performance on testing data set.\subsection{Regularization}In order to access the quality of the prediction produced by the training of ESN, we regularly monitor the actual obtained output weights $\mathbf{W}^{out}$. Large weights indicate that $\mathbf{W}^{out}$ exploits and amplifies tiny differences among the dimension of  $\mathbf{x}(n)$, and can be very sensitive to deviations from the exact conditions in with the network has been trained \cite{mantas}. To counteract this effect the regularization part $\beta \mathbf{I}$ in the ridge regression is used as in Equation  \eqref{eq:wout} .\subsection{Leaking Rate}The leaking rate $\alpha \in \mathbb{R}^{N\times 1}$ of the reservoir nodes can be regarded as the speed of the reservoir update dynamics discretized in time. So the state update  Eq. \eqref{eq:stateUpdate} is modified to Eq. \eqref{eq:stateUpdateWithLeaky}. The values   \begin{equation} \label{eq:stateUpdateWithLeaky}    \textbf{x}(n+1) = (1-\alpha)\textbf{x}(n) + \alpha ( f(\textbf{Wx}(n) +  \textbf{W}^{in}\textbf{u}(n+1) + \mathbf{B} ) )  \end{equation} Each component of the one dimensional vector can be optimized for the better performance of the network however this increases the number of parameters that needs to be optimized linearly with the size of Network. Therefore to make the tuning process easier the one dimensional vector is divided into three regions so that it has three different leaking rates or scaling factors. The entire vector $\alpha$ consists and the three scaling factor are the parameters that needs to be optimized.\subsection{Cross Validation}\indent \indent	In length of available timeseries data is $550$ out of which we leave out the last $31$ data points for the testing purpose once the training phase of ESN is completed. Rest of the data points are used for training purpose.\\	Optimizing a parameter for a reservoir using same set of training and validation data might lead to an overfitting problem. 	    Overfitted model performs well on validation set of data. However, when this overfitted model is tested on new set of testing data, its performance is very poor. Therefore, to avoid such a disaster in performance we have used a concept of cross-validation.	\\Since the length of the training is only $519$ removing a part of it for the validation part poses a risk of underfitting.  By reducing the amount of data for training we risk of not capturing the important pattern/trends in the data set, which in turn increased the error induced by bias. So we require a cross validation method scheme which leaves ample of data for training and ample of data for validation. In our case, K-fold cross validation is first choice because it exactly satisfies previously stated requirement.\subsubsection{K-fold cross validation}\label{kfold}    In K-fold cross validation, the available signals are split into K parts. Out of those K parts one of them is used for validation purpose and rest K-1 split sub-signals are concatenated in original order to form a single signal which is then used for training. This is repeated K times such that all the K splits are used as validation signal exactly once. The overall estimate  error of the cross validation, $\textbf{CV}_k$, is average fo the error in each fold:  \begin{equation}	 \mathbf{CV}_k = \frac{1}{k} \sum^k_{i=1}{NRMSE_i}	 \label{eq:crossValidaion}	 \end{equation}\subsubsection{Computational Efficiency in K-fold cross validation}As mentioned in section \ref{kfold}, the input signal is divided into K splits for the cross validation and in each fold of cross validation loop, (K-1) splits are used for training and the remaining as validation signal. However this process of redoing the similar steps multiple times which can be computationally expensive for longer input signal or higher values of K. Therefore, to decrease the computational cost of training the network, instead of splitting input data in K fold and doing cross validation steps, we feed the entire input signal to the state update equation \eqref{eq:stateUpdateWithLeaky} in a loop that runs from  $i = 1,\hdots,{n_{max}}$ ( ${n_{max}}$ is the length of the input signal). In each iteration $i$ of loop, we  use state vector \textbf{x} of the previous iteration (i.e. at time $i-1$) which goes through a nonlinear transformation to generate state vector at time $i$. For $i = 0$,  we choose the	state vector \textbf{x} as an $N$ dimensional vector of zeros. These state vectors, $\mathbf{x}(n)$ concatenated with input, $\mathbf{u}(n)$ and are stacked in state collection matrix $\mathbf{S} \in \mathbb{R}^{n_{max} \times (N+k)}$.  \\As for ESN, before proceeding to the training the network it is necessary to remove the initial few states (eg. zero state ) which contains initial memory that is not the part of the input. Therefore first $n_0$ rows of matrix $\mathbf{S}$ are discarded. The value of $n_0$ is determined by the observations of the activations of the neurons. By time $n = n_0+1$ , it is save to assume that the effects of the arbitrary initial state have washed away and the network gives a pure reflection of the input signal \cite{jingdai}. \\After removing initial washout from the state collection matrix $\mathbf{S}$, it is divided in K folds in horizontal axis. In each iteration of loop that runs from $i=1,\hdots,K$ the input signal and teacher signal corresponding to one of the set is used as validation data set while the rest of folds and their corresponding teacher signal is used for training the network. This way we only have to compute the states using the entire signal and then reuse those computed states for training and validation during cross validation loop. This way we reduce the computation cost of training the network during cross validation phase.\subsection{Computation of output weights}\indent \indentThe learning of the output weights $\mathbf{W}^{out}$ \eqref{eq:wout} can be phrased as solving a system of linear equation\begin{equation}	\mathbf{W}^{out} \mathbf{S} = D	\label{eq:linearReg}	\end{equation}with respect to $\mathbf{W}^{out}$. Since the goal is to minimize the quadratic error $E(\mathbf{D},\mathbf{W}^{out})$  as in \eqref{eq:nrmse}, to solve \eqref{eq:linearReg} we have used methods for finding least square solutions of overdetermined systems of linear equation i.e linear regression \cite{reservoirComputing}.\\ \\	 $\mathbf{W}^{out}$ is computed as the linear regression of weights of  	    desired outputs, $\mathbf{D}$, on the harvested extended system states during the training phase \textbf{S}. We use Equation \eqref{eq:wout} to compute \textbf{W}$^{out}$.		\subsection{Teacher signals} 